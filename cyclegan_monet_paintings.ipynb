{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='https://raw.githubusercontent.com/dimitreOliveira/MachineLearning/master/Kaggle/I%E2%80%99m%20Something%20of%20a%20Painter%20Myself/banner.png' height=350></center>\n",
    "<p>\n",
    "<h1><center> I’m Something of a Painter Myself </center></h1>\n",
    "<h2><center> Introduction to CycleGAN - Monet paintings </center></h2>\n",
    "\n",
    "#### This notebook is based on the [competition baseline](https://www.kaggle.com/amyjang/monet-cyclegan-tutorial), I just did some refactoring to create helper functions and make everything easier to experiment with, besides that the main contribution is adding the possibility to use data augmentations, as we have very little data here, this will probably help.\n",
    "\n",
    "#### CycleGAN references:\n",
    "- [Git repository](https://junyanz.github.io/CycleGAN/) with many cool informations.\n",
    "- [ArXiv paper](https://arxiv.org/pdf/1703.10593.pdf)\n",
    "- [Understanding and Implementing CycleGAN in TensorFlow](https://hardikbansal.github.io/CycleGANBlog/)\n",
    "\n",
    "\n",
    "### What is CycleGAN?\n",
    "\n",
    "From the authors:\n",
    "> We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G: X → Y, such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F: Y → X and introduce a cycle consistency loss to push F(G(X)) ≈ X (and vice versa).\n",
    "\n",
    "In essence it maps and image to a given domaind, if you are turning horses into zebra the image will be the horse and the domain is the zebras, in our case the photos are the image and the domain are the Monet paintings.\n",
    "\n",
    "#### Turning horses into zebras and zebras into horses\n",
    "![](https://raw.githubusercontent.com/dimitreOliveira/MachineLearning/master/Kaggle/I%E2%80%99m%20Something%20of%20a%20Painter%20Myself/cyclegan_horse-zebra.jpg)\n",
    "\n",
    "#### Turning photos into Monet paintings (our task)\n",
    "![](https://junyanz.github.io/CycleGAN/images/painting2photo.jpg)\n",
    "\n",
    "\n",
    "But it doesn't always works as expected\n",
    "<img src='https://junyanz.github.io/CycleGAN/images/failure_putin.jpg' height=300, width=300>\n",
    "\n",
    "### CycleGAN architecture\n",
    "\n",
    "Looking at the code below may be hard to get what is happening, this image will help the understanding\n",
    "\n",
    "<img src='https://hardikbansal.github.io/CycleGANBlog/images/model.jpg' height=700, width=700>\n",
    "\n",
    "First, we get the regular generator discriminator thing, where the generator tries to generate images that seem to be drawn to the given domain (in the example will be creating zebra images), but it would be possible that the generator generates only the same zebra image or zebra images that do not look like the imputed horse image, this is why the model has a second generator, this second generator uses the first generated image and tries to recreate the original imputed horse image, this way the first generator has to generate zebra images that look like the imputed horse image.\n",
    "\n",
    "#### In the end, you will get 4 sub-models:\n",
    "- A generator that can generate zebras images\n",
    "- A generator that can generate horses images\n",
    "- A discriminator that can identify real zebras images\n",
    "- A discriminator that can identify real horses images\n",
    "\n",
    "\n",
    "#### Let's get to the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import os, random, json, PIL, shutil, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import Model, losses, optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPU configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print(f'Running on TPU {tpu.master()}')\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "\n",
    "REPLICAS = strategy.num_replicas_in_sync\n",
    "print(f'REPLICAS: {REPLICAS}')\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "HEIGHT = 256\n",
    "WIDTH = 256\n",
    "CHANNELS = 3\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "GCS_PATH = KaggleDatasets().get_gcs_path('monet-gan-getting-started')\n",
    "\n",
    "MONET_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/monet_tfrec/*.tfrec'))\n",
    "PHOTO_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/photo_tfrec/*.tfrec'))\n",
    "\n",
    "def count_data_items(filenames):\n",
    "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
    "    return np.sum(n)\n",
    "\n",
    "n_monet_samples = count_data_items(MONET_FILENAMES)\n",
    "n_photo_samples = count_data_items(PHOTO_FILENAMES)\n",
    "\n",
    "print(f'Monet TFRecord files: {len(MONET_FILENAMES)}')\n",
    "print(f'Monet image files: {n_monet_samples}')\n",
    "print(f'Photo TFRecord files: {len(PHOTO_FILENAMES)}')\n",
    "print(f'Photo image files: {n_photo_samples}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliar functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def decode_image(image):\n",
    "    image = tf.image.decode_jpeg(image, channels=CHANNELS)\n",
    "    image = (tf.cast(image, tf.float32) / 127.5) - 1\n",
    "    image = tf.reshape(image, [HEIGHT, WIDTH, CHANNELS])\n",
    "    return image\n",
    "\n",
    "def read_tfrecord(example):\n",
    "    tfrecord_format = {\n",
    "        'image_name': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image':      tf.io.FixedLenFeature([], tf.string),\n",
    "        'target':     tf.io.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, tfrecord_format)\n",
    "    image = decode_image(example['image'])\n",
    "    return image\n",
    "\n",
    "def load_dataset(filenames):\n",
    "    dataset = tf.data.TFRecordDataset(filenames)\n",
    "    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTO)\n",
    "    return dataset\n",
    "\n",
    "def get_gan_dataset(monet_files, photo_files, augment=None, repeat=True, shuffle=True, batch_size=1):\n",
    "\n",
    "    monet_ds = load_dataset(monet_files)\n",
    "    photo_ds = load_dataset(photo_files)\n",
    "\n",
    "    if repeat:\n",
    "        monet_ds = monet_ds.repeat()\n",
    "        photo_ds = photo_ds.repeat()\n",
    "    if shuffle:\n",
    "        monet_ds = monet_ds.shuffle(2048)\n",
    "        photo_ds = photo_ds.shuffle(2048)\n",
    "        \n",
    "    monet_ds = monet_ds.batch(batch_size, drop_remainder=True)\n",
    "    photo_ds = photo_ds.batch(batch_size, drop_remainder=True)\n",
    "    monet_ds = monet_ds.cache()\n",
    "    photo_ds = photo_ds.cache()\n",
    "    monet_ds = monet_ds.prefetch(AUTO)\n",
    "    photo_ds = photo_ds.prefetch(AUTO)\n",
    "    \n",
    "    gan_ds = tf.data.Dataset.zip((monet_ds, photo_ds))\n",
    "    \n",
    "    return gan_ds\n",
    "\n",
    "def display_samples(ds, row, col):\n",
    "    ds_iter = iter(ds)\n",
    "    plt.figure(figsize=(15, int(15*row/col)))\n",
    "    for j in range(row*col):\n",
    "        example_sample = next(ds_iter)\n",
    "        plt.subplot(row,col,j+1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(example_sample[0] * 0.5 + 0.5)\n",
    "    plt.show()\n",
    "        \n",
    "def display_generated_samples(ds, model, n_samples):\n",
    "    ds_iter = iter(ds)\n",
    "    for n_sample in range(n_samples):\n",
    "        example_sample = next(ds_iter)\n",
    "        generated_sample = model.predict(example_sample)\n",
    "        \n",
    "        plt.subplot(121)\n",
    "        plt.title(\"input image\")\n",
    "        plt.imshow(example_sample[0] * 0.5 + 0.5)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(122)\n",
    "        plt.title(\"Generated image\")\n",
    "        plt.imshow(generated_sample[0] * 0.5 + 0.5)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "def predict_and_save(input_ds, generator_model, output_path):\n",
    "    i = 1\n",
    "    for img in input_ds:\n",
    "        prediction = generator_model(img, training=False)[0].numpy() # make predition\n",
    "        prediction = (prediction * 127.5 + 127.5).astype(np.uint8)   # re-scale\n",
    "        im = PIL.Image.fromarray(prediction)\n",
    "        im.save(f'{output_path}{str(i)}.jpg')\n",
    "        i += 1\n",
    "                \n",
    "\n",
    "# Model functions\n",
    "def downsample(filters, size, apply_instancenorm=True, strides=2):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    gamma_init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(L.Conv2D(filters, size, strides=strides, padding='same',\n",
    "                        kernel_initializer=initializer, use_bias=False))\n",
    "\n",
    "    if apply_instancenorm:\n",
    "        result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n",
    "\n",
    "    result.add(L.LeakyReLU())\n",
    "\n",
    "    return result\n",
    "\n",
    "def upsample(filters, size, apply_dropout=False, strides=2):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    gamma_init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(L.Conv2DTranspose(filters, size, strides=strides, padding='same',\n",
    "                                 kernel_initializer=initializer, use_bias=False))\n",
    "\n",
    "    result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n",
    "\n",
    "    if apply_dropout:\n",
    "        result.add(L.Dropout(0.5))\n",
    "\n",
    "    result.add(L.ReLU())\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at a few Monet paintings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "display_samples(load_dataset(MONET_FILENAMES).batch(1), 4, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at a few photo samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "display_samples(load_dataset(PHOTO_FILENAMES).batch(1), 4, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 3\n",
    "\n",
    "def generator_fn():\n",
    "    inputs = L.Input(shape=[HEIGHT, WIDTH, CHANNELS])\n",
    "\n",
    "    down_stack = [\n",
    "        downsample(64, 4, apply_instancenorm=False), # (bs, 128, 128, 64)\n",
    "        downsample(128, 4),                          # (bs, 64, 64, 128)\n",
    "        downsample(256, 4),                          # (bs, 32, 32, 256)\n",
    "        downsample(512, 4),                          # (bs, 16, 16, 512)\n",
    "        downsample(512, 4),                          # (bs, 8, 8, 512)\n",
    "        downsample(512, 4),                          # (bs, 4, 4, 512)\n",
    "        downsample(512, 4),                          # (bs, 2, 2, 512)\n",
    "        downsample(512, 4),                          # (bs, 1, 1, 512)\n",
    "    ]\n",
    "\n",
    "    up_stack = [\n",
    "        upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n",
    "        upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n",
    "        upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n",
    "        upsample(512, 4),                     # (bs, 16, 16, 1024)\n",
    "        upsample(256, 4),                     # (bs, 32, 32, 512)\n",
    "        upsample(128, 4),                     # (bs, 64, 64, 256)\n",
    "        upsample(64, 4),                      # (bs, 128, 128, 128)\n",
    "    ]\n",
    "\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    last = L.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
    "                             strides=2,\n",
    "                             padding='same',\n",
    "                             kernel_initializer=initializer,\n",
    "                             activation='tanh') # (bs, 256, 256, 3)\n",
    "\n",
    "    x = inputs\n",
    "\n",
    "    # Downsampling through the model\n",
    "    skips = []\n",
    "    for down in down_stack:\n",
    "        x = down(x)\n",
    "        skips.append(x)\n",
    "\n",
    "    skips = reversed(skips[:-1])\n",
    "\n",
    "    # Upsampling and establishing the skip connections\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        x = L.Concatenate()([x, skip])\n",
    "\n",
    "    x = last(x)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "def discriminator_fn():\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    gamma_init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "\n",
    "    inp = L.Input(shape=[HEIGHT, WIDTH, CHANNELS], name='input_image')\n",
    "\n",
    "    x = inp\n",
    "\n",
    "    down1 = downsample(64, 4, False)(x) # (bs, 128, 128, 64)\n",
    "    down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n",
    "    down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)\n",
    "\n",
    "    zero_pad1 = L.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n",
    "    conv = L.Conv2D(512, 4, strides=1,\n",
    "                    kernel_initializer=initializer,\n",
    "                    use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n",
    "\n",
    "    norm1 = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(conv)\n",
    "\n",
    "    leaky_relu = L.LeakyReLU()(norm1)\n",
    "\n",
    "    zero_pad2 = L.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n",
    "\n",
    "    last = L.Conv2D(1, 4, strides=1,\n",
    "                    kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n",
    "\n",
    "    return Model(inputs=inp, outputs=last)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model (CycleGAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    monet_generator = generator_fn() # transforms photos to Monet-esque paintings\n",
    "    photo_generator = generator_fn() # transforms Monet paintings to be more like photos\n",
    "\n",
    "    monet_discriminator = discriminator_fn() # differentiates real Monet paintings and generated Monet paintings\n",
    "    photo_discriminator = discriminator_fn() # differentiates real photos and generated photos\n",
    "\n",
    "\n",
    "class CycleGan(Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        monet_generator,\n",
    "        photo_generator,\n",
    "        monet_discriminator,\n",
    "        photo_discriminator,\n",
    "        lambda_cycle=10,\n",
    "    ):\n",
    "        super(CycleGan, self).__init__()\n",
    "        self.m_gen = monet_generator\n",
    "        self.p_gen = photo_generator\n",
    "        self.m_disc = monet_discriminator\n",
    "        self.p_disc = photo_discriminator\n",
    "        self.lambda_cycle = lambda_cycle\n",
    "        \n",
    "    def compile(\n",
    "        self,\n",
    "        m_gen_optimizer,\n",
    "        p_gen_optimizer,\n",
    "        m_disc_optimizer,\n",
    "        p_disc_optimizer,\n",
    "        gen_loss_fn,\n",
    "        disc_loss_fn,\n",
    "        cycle_loss_fn,\n",
    "        identity_loss_fn\n",
    "    ):\n",
    "        super(CycleGan, self).compile()\n",
    "        self.m_gen_optimizer = m_gen_optimizer\n",
    "        self.p_gen_optimizer = p_gen_optimizer\n",
    "        self.m_disc_optimizer = m_disc_optimizer\n",
    "        self.p_disc_optimizer = p_disc_optimizer\n",
    "        self.gen_loss_fn = gen_loss_fn\n",
    "        self.disc_loss_fn = disc_loss_fn\n",
    "        self.cycle_loss_fn = cycle_loss_fn\n",
    "        self.identity_loss_fn = identity_loss_fn\n",
    "        \n",
    "    def train_step(self, batch_data):\n",
    "        real_monet, real_photo = batch_data\n",
    "        \n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # photo to monet back to photo\n",
    "            fake_monet = self.m_gen(real_photo, training=True)\n",
    "            cycled_photo = self.p_gen(fake_monet, training=True)\n",
    "\n",
    "            # monet to photo back to monet\n",
    "            fake_photo = self.p_gen(real_monet, training=True)\n",
    "            cycled_monet = self.m_gen(fake_photo, training=True)\n",
    "\n",
    "            # generating itself\n",
    "            same_monet = self.m_gen(real_monet, training=True)\n",
    "            same_photo = self.p_gen(real_photo, training=True)\n",
    "\n",
    "            # discriminator used to check, inputing real images\n",
    "            disc_real_monet = self.m_disc(real_monet, training=True)\n",
    "            disc_real_photo = self.p_disc(real_photo, training=True)\n",
    "\n",
    "            # discriminator used to check, inputing fake images\n",
    "            disc_fake_monet = self.m_disc(fake_monet, training=True)\n",
    "            disc_fake_photo = self.p_disc(fake_photo, training=True)\n",
    "\n",
    "            # evaluates generator loss\n",
    "            monet_gen_loss = self.gen_loss_fn(disc_fake_monet)\n",
    "            photo_gen_loss = self.gen_loss_fn(disc_fake_photo)\n",
    "\n",
    "            # evaluates total cycle consistency loss\n",
    "            total_cycle_loss = self.cycle_loss_fn(real_monet, cycled_monet, self.lambda_cycle) + self.cycle_loss_fn(real_photo, cycled_photo, self.lambda_cycle)\n",
    "\n",
    "            # evaluates total generator loss\n",
    "            total_monet_gen_loss = monet_gen_loss + total_cycle_loss + self.identity_loss_fn(real_monet, same_monet, self.lambda_cycle)\n",
    "            total_photo_gen_loss = photo_gen_loss + total_cycle_loss + self.identity_loss_fn(real_photo, same_photo, self.lambda_cycle)\n",
    "\n",
    "            # evaluates discriminator loss\n",
    "            monet_disc_loss = self.disc_loss_fn(disc_real_monet, disc_fake_monet)\n",
    "            photo_disc_loss = self.disc_loss_fn(disc_real_photo, disc_fake_photo)\n",
    "\n",
    "        # Calculate the gradients for generator and discriminator\n",
    "        monet_generator_gradients = tape.gradient(total_monet_gen_loss,\n",
    "                                                  self.m_gen.trainable_variables)\n",
    "        photo_generator_gradients = tape.gradient(total_photo_gen_loss,\n",
    "                                                  self.p_gen.trainable_variables)\n",
    "\n",
    "        monet_discriminator_gradients = tape.gradient(monet_disc_loss,\n",
    "                                                      self.m_disc.trainable_variables)\n",
    "        photo_discriminator_gradients = tape.gradient(photo_disc_loss,\n",
    "                                                      self.p_disc.trainable_variables)\n",
    "\n",
    "        # Apply the gradients to the optimizer\n",
    "        self.m_gen_optimizer.apply_gradients(zip(monet_generator_gradients,\n",
    "                                                 self.m_gen.trainable_variables))\n",
    "\n",
    "        self.p_gen_optimizer.apply_gradients(zip(photo_generator_gradients,\n",
    "                                                 self.p_gen.trainable_variables))\n",
    "\n",
    "        self.m_disc_optimizer.apply_gradients(zip(monet_discriminator_gradients,\n",
    "                                                  self.m_disc.trainable_variables))\n",
    "\n",
    "        self.p_disc_optimizer.apply_gradients(zip(photo_discriminator_gradients,\n",
    "                                                  self.p_disc.trainable_variables))\n",
    "        \n",
    "        return {\n",
    "            'monet_gen_loss': total_monet_gen_loss,\n",
    "            'photo_gen_loss': total_photo_gen_loss,\n",
    "            'monet_disc_loss': monet_disc_loss,\n",
    "            'photo_disc_loss': photo_disc_loss\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    # Discriminator loss {0: fake, 1: real} (The discriminator loss outputs the average of the real and generated loss)\n",
    "    def discriminator_loss(real, generated):\n",
    "        real_loss = losses.BinaryCrossentropy(from_logits=True, reduction=losses.Reduction.NONE)(tf.ones_like(real), real)\n",
    "\n",
    "        generated_loss = losses.BinaryCrossentropy(from_logits=True, reduction=losses.Reduction.NONE)(tf.zeros_like(generated), generated)\n",
    "\n",
    "        total_disc_loss = real_loss + generated_loss\n",
    "\n",
    "        return total_disc_loss * 0.5\n",
    "    \n",
    "    # Generator loss\n",
    "    def generator_loss(generated):\n",
    "        return losses.BinaryCrossentropy(from_logits=True, reduction=losses.Reduction.NONE)(tf.ones_like(generated), generated)\n",
    "    \n",
    "    \n",
    "    # Cycle consistency loss (measures if original photo and the twice transformed photo to be similar to one another)\n",
    "    with strategy.scope():\n",
    "        def calc_cycle_loss(real_image, cycled_image, LAMBDA):\n",
    "            loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
    "\n",
    "            return LAMBDA * loss1\n",
    "\n",
    "    # Identity loss (compares the image with its generator (i.e. photo with photo generator))\n",
    "    with strategy.scope():\n",
    "        def identity_loss(real_image, same_image, LAMBDA):\n",
    "            loss = tf.reduce_mean(tf.abs(real_image - same_image))\n",
    "            return LAMBDA * 0.5 * loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    # Create generators\n",
    "    monet_generator_optimizer = optimizers.Adam(2e-4, beta_1=0.5)\n",
    "    photo_generator_optimizer = optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "    # Create discriminators\n",
    "    monet_discriminator_optimizer = optimizers.Adam(2e-4, beta_1=0.5)\n",
    "    photo_discriminator_optimizer = optimizers.Adam(2e-4, beta_1=0.5)\n",
    "    \n",
    "    # Create GAN\n",
    "    gan_model = CycleGan(monet_generator, photo_generator, \n",
    "                         monet_discriminator, photo_discriminator)\n",
    "\n",
    "    gan_model.compile(m_gen_optimizer=monet_generator_optimizer,\n",
    "                      p_gen_optimizer=photo_generator_optimizer,\n",
    "                      m_disc_optimizer=monet_discriminator_optimizer,\n",
    "                      p_disc_optimizer=photo_discriminator_optimizer,\n",
    "                      gen_loss_fn=generator_loss,\n",
    "                      disc_loss_fn=discriminator_loss,\n",
    "                      cycle_loss_fn=calc_cycle_loss,\n",
    "                      identity_loss_fn=identity_loss)\n",
    "    \n",
    "\n",
    "history = gan_model.fit(get_gan_dataset(MONET_FILENAMES, PHOTO_FILENAMES, batch_size=BATCH_SIZE), \n",
    "                        steps_per_epoch=(n_monet_samples//BATCH_SIZE),\n",
    "                        epochs=EPOCHS,\n",
    "                        verbose=2).history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "display_generated_samples(load_dataset(PHOTO_FILENAMES).batch(1), monet_generator, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "os.makedirs('../images/') # Create folder to save generated images\n",
    "\n",
    "predict_and_save(load_dataset(PHOTO_FILENAMES).batch(1), monet_generator, '../images/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "shutil.make_archive('/kaggle/working/images/', 'zip', '../images')\n",
    "\n",
    "print(f\"Generated samples: {len([name for name in os.listdir('../images/') if os.path.isfile(os.path.join('../images/', name))])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
